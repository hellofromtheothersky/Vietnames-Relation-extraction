# # -*- coding: utf-8 -*-
# """generate_features.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1-d9wXts8YF0tJXE0BAxI96ik4VZ-aX13
# """

# # Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive/')
# # %cd /content/drive/MyDrive/thesis-relation-extraction-vn

# ! pip3 install py_vncorenlp

import py_vncorenlp
import pandas as pd
import numpy as np
import re
import json
import pickle
import os

import networkx as nx
from networkx import NetworkXNoPath
from networkx import NodeNotFound

def get_shortest_path(edges, a, b):
    edges=[[x[0], x[1]] for x in edges]
    und_graph = nx.Graph(edges)
    try:
        und_path=nx.shortest_path(und_graph, source=a, target=b)
    except NetworkXNoPath:
        return [], None
    except NodeNotFound:
        return [], None
    else:
        try: #wwtffffff
          edges=[edges[i] for i in und_path]
          di_graph=nx.DiGraph(edges)
          for node in und_path:
              try:
                  di_path1=nx.shortest_path(di_graph, source=a, target=node)
                  di_path2=nx.shortest_path(di_graph, source=b, target=node)
              except NetworkXNoPath:
                  pass
              else:
                  return und_path, node
        except:
            pass
        return [], None

def split_entity_and_create_edges(org_sentence):
    org_sentence=' '.join(re.findall('<e1>|<\/e1>|<e2>|<\/e2>|\w+', org_sentence)) # space processing -> still remaining entity mark
    sentence=re.sub('<e1>|<\/e1>|<e2>|<\/e2>', '', org_sentence)
    #tokenize
    org_tokens=org_sentence.split()
    # print(sentence)
    words_analysis=vncorenlp_md.annotate_text(sentence)[0]
    tokens=[x['wordForm'] for x in words_analysis]
    #create dependency edges
    edges=[[x['index']-1, x['head']-1, x['depLabel'], x['posTag'], x['wordForm']] for x in words_analysis]

    seps=['<e1>', '</e1>', '<e2>', '</e2>']
    epos_temp=[] # start, end of two entities
    j=0
    i=0
    # print(tokens)
    # print(org_tokens)
    while i in range(len(org_tokens)):
        # print(org_tokens[i])
        if org_tokens[i] in seps:
            epos_temp.append(j)
        else:
            k=tokens[j].count('_') # the number of two token merged into one
            i+=k
            j+=1
        i+=1

    epos=[[-1, -1], [-1, -1]]
    epos[0][0]=epos_temp[0]
    epos[0][1]=epos_temp[1]-1
    epos[1][0]=epos_temp[2]
    epos[1][1]=epos_temp[3]-1
    return {'sentence': ' '.join(tokens), 'epos': epos, 'tokens': tokens, 'edges': edges}


def split_entity_and_create_edges_ver2(org_sentence):
    org_sentence=' '.join(re.findall('<e1>|<\/e1>|<e2>|<\/e2>|\w+', org_sentence)) # space processing -> still remaining entity mark
    sentence=re.sub('<e1>', '(', org_sentence)
    sentence=re.sub('<e2>', '(', sentence)
    sentence=re.sub('<\/e1>', ')', sentence)
    sentence=re.sub('<\/e2>', ')', sentence)
    # print(sentence)
    #tokenize
    # print(sentence)
    words_analysis=vncorenlp_md.annotate_text(sentence)[0]
    tokens=[x['wordForm'] for x in words_analysis if x['wordForm']!='(' and x['wordForm']!=')']
    # print(tokens)
    #create dependency edges
    edges=[[x['index']-1, x['head']-1, x['depLabel'], x['posTag'], x['wordForm']] for x in words_analysis]
    # print(edges)

    #sentence have special character (, ) -> edges have this tokens -> remove (, ) from the edges -> modify the index in edges
    precessed_edges=[]
    mapping={-1: -1}
    delta=0
    epos_tmp=[]
    for edge in edges:
        token=edge[-1] #get the text
        if token=='(' or token==')':
            delta=delta+1
            if token =='(':
                epos_tmp.append(edge[0]+1)
            else:
                epos_tmp.append(edge[0]-1)
        else:
            mapping[edge[0]]=edge[0]-delta
            precessed_edges.append(edge)

    # print(precessed_edges)
    # print(mapping)
    for i in range(len(precessed_edges)):
        precessed_edges[i][0]=mapping[precessed_edges[i][0]]
        precessed_edges[i][1]=mapping[precessed_edges[i][1]]

    epos=[[-1, -1], [-1, -1]]
    # print(epos_tmp)
    epos[0][0]=mapping[epos_tmp[0]]
    epos[0][1]=mapping[epos_tmp[1]]
    epos[1][0]=mapping[epos_tmp[2]]
    epos[1][1]=mapping[epos_tmp[3]]

    return {'sentence': ' '.join(tokens), 'epos': epos, 'tokens': tokens, 'edges': precessed_edges}


def create_relative_distance(sentence_data):
    e1_distance=[]
    e2_distance=[]
    e1_pos=sentence_data['epos'][0]
    e2_pos=sentence_data['epos'][1]
    for i in range(len(sentence_data['tokens'])):
        if i<e1_pos[0]:
            e1_distance.append(i-e1_pos[0])
        elif i>e1_pos[1]:
            e1_distance.append(i-e1_pos[1])
        else:
            e1_distance.append(0)
        if i<e2_pos[0]:
            e2_distance.append(i-e2_pos[0])
        elif i>e2_pos[1]:
            e2_distance.append(i-e2_pos[1])
        else:
            e2_distance.append(0)
    return e1_distance, e2_distance


def path_between_2entity(sentence_data, edges):
    first_e1_pos=sentence_data['epos'][0][0]
    first_e2_pos=sentence_data['epos'][1][0]
    path, grand_parent=get_shortest_path(edges, first_e1_pos, first_e2_pos)
    path_array=['N']*len(edges)
    try:
        grand_parent_pos=path.index(grand_parent)
    except ValueError:
        pass
    else:
        for i, node in enumerate(path):
            path_array[node]=i-grand_parent_pos
    finally:
        return path_array


SKIP=[] #dev only
def generate_features(sentences):
    # tokenize the text using VNNLPCORE, relative distance, shortest dependency path with grammar relation
    sentences_data=[]
    for i, s in enumerate(sentences):
        try:
            sentence_data = split_entity_and_create_edges(s)
        except IndexError:
            print('TRY keep entity mark when tokenize for')
            print(s)
            try:
                sentence_data=split_entity_and_create_edges_ver2(s)
            except:
                print('SKIP for')
                print(s)
                sentence_data=None
        except:
            print('SKIP for')
            print(s)
            sentence_data=None
        finally:
            if sentence_data!=None:
                sentences_data.append(sentence_data)
            else:
                SKIP.append(i)


    e1_distance, e2_distance, grammar, shortest_path=[], [], [], []
    for sentence_data in sentences_data:
        #distant
        distance=create_relative_distance(sentence_data)
        e1_distance.append(distance[0])
        e2_distance.append(distance[1])

        edges=sentence_data['edges']
        grammar.append(edges)
        shortest_path.append(path_between_2entity(sentence_data, edges))

    sentences=[x['sentence'] for x in sentences_data]
    return sentences, e1_distance, e2_distance, grammar, shortest_path

if __name__ == "__main__":
    vncorenlp_md = py_vncorenlp.VnCoreNLP(save_dir='/content/drive/MyDrive/thesis-relation-extraction-vn/vncorenlp/')
    
    os.chdir('/content/drive/MyDrive/thesis-relation-extraction-vn')

    for type in ['train', 'test']:
        SKIP=[]
        data = pd.read_csv('data/'+type+'.csv')

        sentences, e1_distance, e2_distance, grammar, shortest_path=generate_features(data['sentence_vi'])
        label=list(data['relationship'])

        label=[x for i, x in enumerate(label) if i not in SKIP] #dev only

        with open('data/'+type+'_features.json', 'w') as w:
            json.dump({'sentences': sentences,
                    'e1_distance': e1_distance,
                    'e2_distance': e2_distance,
                    'grammar': grammar,
                    'shortest_path': shortest_path,
                    'label': label
                    }, w)