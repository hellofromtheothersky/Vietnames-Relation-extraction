# # -*- coding: utf-8 -*-
# """method2_encode_features.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/11y_7_2THDqYdUEROmkp-0kN7MLRowBIv
# """

# # Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive/')
# # %cd /content/drive/MyDrive/thesis-relation-extraction-vn

# !pip install transformers

def pre_processing_for_method2(sentences, e1_distances, e2_distances, grammars, shortest_paths):
    sentences2_tokens, e1_distances2, e2_distances2, postags=[], [], [], []
    dp_types, dp_directions=[], []


    for i in range(len(sentences)):
        sentences2_tokens.append([])
        e1_distances2.append([])
        e2_distances2.append([])
        postags.append([])
        dp_types.append([])
        dp_directions.append([])

        sentences_tokens=sentences[i].split()

        shortest_path=[[i, x] for i, x in enumerate(shortest_paths[i]) if x != 'N']
        shortest_path.sort(key=lambda x: x[1]) # sort by the value, the x[0] is index of the value on the org sentence

        for k in range(len(shortest_path)):
            j=shortest_path[k][0]

            sentences2_tokens[i].append(sentences_tokens[j])
            e1_distances2[i].append(e1_distances[i][j])
            e2_distances2[i].append(e2_distances[i][j])
            postags[i].append(grammars[i][j][3])
            if grammar[i][j][2]!='root':
                dp_types[i].append(grammar[i][j][2])

            if shortest_path[k][1]>0:
                dp_directions[i].append(1)
            elif shortest_path[k][1]<0:
                dp_directions[i].append(2)


    sentences2=[' '.join(x) for x in sentences2_tokens]
    return sentences2, e1_distances2, e2_distances2, postags, dp_types, dp_directions

import torch
from transformers import AutoTokenizer

import os

import pandas as pd
import numpy as np
import re
import json
import pickle
import torch

from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

def flatten_extend(matrix):
    flat_list = []
    for row in matrix:
        flat_list.extend(row)
    return flat_list


class RE_DataEncoder2():
    def __init__(self, vocab_size, max_len, org_max_len, postags, dp_types, label_train):
        self.max_len=max_len
        self.org_max_len=org_max_len
        self.vocab_size=vocab_size

        #for the pos tag
        pos_tag_unique=set(flatten_extend(postags))
        self.postag2idx={v:i+1 for i, v in enumerate(pos_tag_unique)}

        #for the dp_types
        dp_types_unique=set(flatten_extend(dp_types))
        self.dp_type2idx={v:i+1 for i, v in enumerate(dp_types_unique)}

        #for the label
        self.lbencoder = LabelEncoder().fit(label_train)
        self.dict_labels={i: w for i, w in enumerate(list(self.lbencoder.classes_))}


    def encode_sentences(self, sentences):
        input_ids=[]
        mask_ids=[]
        for sentence in sentences:
            input_id = list(phobert_tokenizer.encode(sentence))[1:-1] # remove CLS and SEP token
            mask_id = [1]*len(input_id)
            while(len(input_id)<self.max_len):
                input_id.append(0)
                mask_id.append(0)
            input_id = input_id[:self.max_len]
            mask_id = mask_id[:self.max_len]

            input_ids.append(input_id)
            mask_ids.append(mask_id)

        return np.array(input_ids), np.array(mask_ids)


    def encode_distances(self, distances):
        distances=pad_sequences(distances, maxlen=self.max_len, value=999, padding='post')
        distances+=self.org_max_len+1
        distances[distances==1000+self.org_max_len]=0
        return distances


    def encode_postags(self, postags):
        for i in range(len(postags)):
            for j in range(len(postags[i])):
                try:
                    postags[i][j]=self.postag2idx[postags[i][j]]
                except KeyError:
                  postags[i][j]=1

        postags=pad_sequences(postags, maxlen=self.max_len, value=0, padding='post')
        return postags


    def encode_dp_types(self, dp_types):
        print(dp_types)

        for i in range(len(dp_types)):
            for j in range(len(dp_types[i])):
                try:
                    dp_types[i][j]=self.dp_type2idx[dp_types[i][j]]
                except KeyError:
                  dp_types[i][j]=1


        print(dp_types)

        dp_types=pad_sequences(dp_types, maxlen=self.max_len, value=0, padding='post')
        return dp_types


    def encode(self, sentences, e1_distances, e2_distances, postags, dp_types, dp_directions):
        input_ids, mask_ids=self.encode_sentences(sentences)

        #relative distance
        e1_distances=self.encode_distances(e1_distances)
        e2_distances=self.encode_distances(e2_distances)

        #postags
        postags=self.encode_postags(postags)

        #dp_types
        dp_types=self.encode_dp_types(dp_types)

        #dp_direction
        dp_directions=pad_sequences(dp_directions, maxlen=self.max_len, value=0, padding='post')


        return input_ids, mask_ids, e1_distances, e2_distances, postags, dp_types, dp_directions


    def encode_label(self, label_name):
        label = self.lbencoder.transform(label_name)
        label = to_categorical(label, num_classes=len(set(label_name))) # from keras.utils.np_utils
        label = np.array(label)
        return label

if __name__ == "__main__":
    phobert_tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2")

    os.chdir('/content/drive/MyDrive/thesis-relation-extraction-vn')

    vocab_size=4524
    max_len = 12
    org_max_len = 90

    for type in ['train', 'test']:
        with open('data/'+type+'_features.json', 'r') as r:
            data=json.load(r)
        sentences=data['sentences']
        e1_distance=data['e1_distance']
        e2_distance=data['e2_distance']
        grammar=data['grammar']
        shortest_path=data['shortest_path']
        label=data['label']

        sentences, e1_distance, e2_distance, postags, dp_types, dp_directions = pre_processing_for_method2(sentences, e1_distance, e2_distance, grammar, shortest_path)

        if type=='train':
            Encoder = RE_DataEncoder2(vocab_size, max_len, org_max_len, postags, dp_types, label)

        input_ids_np , mask_ids_np, e1_distance_np, e2_distance_np, postags_np, dp_types_np, dp_direction_np = Encoder.encode(sentences,
                                                                                                    e1_distance,
                                                                                                    e2_distance,
                                                                                                    postags,
                                                                                                    dp_types,
                                                                                                    dp_directions
                                                                                                    )

        label_np=Encoder.encode_label(label)

        all_features=np.array([input_ids_np , mask_ids_np, e1_distance_np,  e2_distance_np, postags_np, dp_types_np, dp_direction_np])
        np.save('data/X_'+type+'_method2.npy', all_features)
        np.save('data/y_'+type+'_method2.npy', label_np)

        #save encoder class
        with open('data/data_encoder_method2.obj', 'wb') as f:
            pickle.dump(Encoder, f)

# #FIND MAX LEN
# import seaborn as sns
# print(max([len(x.split()) for x in sentences]))
# sns.histplot([len(x.split()) for x in sentences]);

# #FIND WORD SIZE
# tokenizer = Tokenizer(num_words=1, filters='!"#$%&()*+,-./:;=?@[]^`{|}~', lower=True, oov_token=1) # bo dau _
# tokenizer.fit_on_texts(sentences)
# word_index = tokenizer.word_index
# word_size = len(word_index)
# word_size

# #FIND MAX LEN
# import seaborn as sns
# print(max([len(x.split()) for x in sentences]))
# sns.histplot([len(x.split()) for x in sentences]);