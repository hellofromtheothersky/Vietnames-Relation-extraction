# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18GRFqyGOwW6kO3-22W1932ohtKHKuB2Q
"""

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/thesis-relation-extraction-vn/

from process_data import RE_DataEncoder
from BaseModel import BaseModel

import numpy as np
import seaborn as sns
import pickle
import torch

from keras.layers import Dense, Embedding, Conv1D

from tensorflow.keras.layers import Embedding, Dense, Dropout, Input, concatenate
from tensorflow.keras.layers import GlobalMaxPool1D
from keras.models import Model

# from keras.layers import Layer
# from transformers import TFBertModel
# from tensorflow.keras import Model
# import json


# class CustomPhoBertModel(Layer):
#     def __init__(self, **kwargs):
#         super(CustomPhoBertModel, self).__init__(**kwargs)
#         self.bert_model = phobert_model_tf


#     def call(self, input_ids, input_mask):
#         input_ids=tf.cast(input_ids, dtype="int32")
#         input_mask=tf.cast(input_mask, dtype="int32")

#         print(input_ids)
#         # with open('phobert_emb.json', 'r') as rf:
#         #     emb_from_ids=json.load(rf)
#         # key_id='_'.join(input_ids.toList)
#         # if key_id in emb_from_ids:
#         #     result=emb_from_ids[key_id]
#         # else:
#         #     result = self.bert_model(input_ids=input_ids, attention_mask=input_mask)["last_hidden_state"]
#         #     emb_from_ids[key_id]=result
#         #     with open('phobert_emb.json', 'r') as rf:
#         #         json.dump(emb_from_ids, rf)
#         result = self.bert_model(input_ids=input_ids, attention_mask=input_mask)["last_hidden_state"]
#         return result

# # Example usage

# input_ids = tf.keras.Input(shape=(128,))
# attention_mask = tf.keras.Input(shape=(128,))

# phobert_model = CustomPhoBertModel()

class CNN_model(BaseModel):
    def build_model(self, using=['word_emb', 'position_emb', 'gram_emb', 'sp_emb']):
        # input_ids = Input(shape=(max_len,), name='sentence')
        # input_masks = Input(shape=(max_len,), name='masks')
        # phobertttt=phobert_model_tf(input_ids, input_masks)
        # phobert_model = CustomPhoBertModel()
        # phobert_emb = phobert_model(input_ids, input_masks)
        phobert_emb=Input(shape=(max_len, 768,), name='sentence')

        # embed_sentence_phobert = Embedding(input_dim=11665, #Encoder.word_size+1
        #                             output_dim=768, #cần tương thích vs tham số weights bên dưới
        #                             input_length=max_len,
        #                             weights = [emb_matrix],
        #                             name='sentence_phobert', mask_zero=True)(input_sentence)

        input_e1_pos = Input(shape=(max_len,), name='e1_position')
        embed_e1_pos = Embedding(172,200, input_length=max_len, mask_zero=True)(input_e1_pos)
        input_e2_pos = Input(shape=(max_len,), name='e2_position')
        embed_e2_pos = Embedding(169,200, input_length=max_len, mask_zero=True)(input_e2_pos)

        input_grammar = Input(shape=(max_len,), name='grammar_relation')
        embed_grammar = Embedding(31,100, input_length=max_len, mask_zero=True)(input_grammar)

        input_sp = Input(shape=(max_len,), name='shortest_path')
        embed_sp = Embedding(107, 500, input_length=max_len, mask_zero=True)(input_sp)

        input_list=[]
        if 'word_emb' in using:
            input_list.append(phobert_emb)
        if 'position_emb' in using:
            input_list.extend([embed_e1_pos, embed_e2_pos])
        if 'gram_emb' in using:
            input_list.append(embed_grammar)
        if 'sp_emb' in using:
            input_list.append(embed_sp)

        # input_list.append(phobert_emb)

        visible = concatenate(input_list)
        interp = Conv1D(filters=200, kernel_size=3, activation='relu')(visible)
        interp = GlobalMaxPool1D()(interp)
        interp = Dropout(0.2)(interp)
        output = Dense(19, activation='softmax')(interp)
        self.model = Model(inputs=[phobert_emb, input_e1_pos, input_e2_pos, input_grammar, input_sp], outputs=output)

# np.unique(X_train[5].flatten())

with open('data/data_encoder.obj', 'rb') as f:
    Encoder=pickle.load(f)

vocab_size=Encoder.vocab_size
max_len= Encoder.max_len

X_train = np.load('data/X_train.npy')
y_train = np.load('data/y_train.npy')
X_test = np.load('data/X_test.npy')
y_test = np.load('data/y_test.npy')

sentence_emb_train=torch.load('data/sentence_emb_train_tensor.pt')
sentence_emb_test=torch.load('data/sentence_emb_test_tensor.pt')

cnn=CNN_model()
cnn.build_model()

cnn.model.summary()

from keras.utils import plot_model
plot_model(cnn.model)

cnn.train_model([sentence_emb_train.cpu().detach().numpy(), X_train[2], X_train[3], X_train[4], X_train[5]], y_train, epochs=3)

cnn.evaluate([sentence_emb_test.cpu().detach().numpy(), X_test[2], X_test[3], X_test[4], X_test[5]], y_test, Encoder.dict_labels)

cnn.save_model("DK_first_all_3ep")