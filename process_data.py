# -*- coding: utf-8 -*-
"""process_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rC1Qf3uCIpoFJt9DtMU5GXMTLztk2fSL
"""

# from google.colab import drive
# drive.mount('/content/drive/')

# %cd /content/drive/MyDrive/thesis-relation-extraction-vn/
# ! pip3 install py_vncorenlp
# import py_vncorenlp
# vncorenlp_md = py_vncorenlp.VnCoreNLP(save_dir='/content/drive/MyDrive/thesis-relation-extraction-vn/vncorenlp/')

# %cd /content/drive/MyDrive/thesis-relation-extraction-vn/

import pandas as pd
import numpy as np
import re
import json
import pickle

# import spacy
# from spacy import displacy
# nlp = spacy.load("en_core_web_sm")

import networkx as nx
from networkx import NetworkXNoPath
from networkx import NodeNotFound

from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

def split_entity_and_create_edges(org_sentence):
    org_sentence=' '.join(re.findall('<e1>|<\/e1>|<e2>|<\/e2>|\w+', org_sentence)) # space processing -> still remaining entity mark
    sentence=re.sub('<e1>|<\/e1>|<e2>|<\/e2>', '', org_sentence)
    #tokenize
    org_tokens=org_sentence.split()
    # print(sentence)
    words_analysis=vncorenlp_md.annotate_text(sentence)[0]
    tokens=[x['wordForm'] for x in words_analysis]
    #create dependency edges
    edges=[[x['index']-1, x['head']-1, x['depLabel'], x['wordForm']] for x in words_analysis]

    seps=['<e1>', '</e1>', '<e2>', '</e2>']
    epos_temp=[] # start, end of two entities
    j=0
    i=0
    # print(tokens)
    # print(org_tokens)
    while i in range(len(org_tokens)):
        # print(org_tokens[i])
        if org_tokens[i] in seps:
            epos_temp.append(j)
        else:
            k=tokens[j].count('_') # the number of two token merged into one
            i+=k
            j+=1
        i+=1

    epos=[[-1, -1], [-1, -1]]
    epos[0][0]=epos_temp[0]
    epos[0][1]=epos_temp[1]-1
    epos[1][0]=epos_temp[2]
    epos[1][1]=epos_temp[3]-1
    return {'sentence': ' '.join(tokens), 'epos': epos, 'tokens': tokens, 'edges': edges}


def split_entity_and_create_edges_ver2(org_sentence):
    org_sentence=' '.join(re.findall('<e1>|<\/e1>|<e2>|<\/e2>|\w+', org_sentence)) # space processing -> still remaining entity mark
    sentence=re.sub('<e1>', '(', org_sentence)
    sentence=re.sub('<e2>', '(', sentence)
    sentence=re.sub('<\/e1>', ')', sentence)
    sentence=re.sub('<\/e2>', ')', sentence)
    # print(sentence)
    #tokenize
    # print(sentence)
    words_analysis=vncorenlp_md.annotate_text(sentence)[0]
    tokens=[x['wordForm'] for x in words_analysis ]
    # print(tokens)
    #create dependency edges
    edges=[[x['index']-1, x['head']-1, x['depLabel'], x['wordForm']] for x in words_analysis]
    # print(edges)

    #sentence have special character (, ) -> edges have this tokens -> remove (, ) from the edges -> modify the index in edges
    precessed_edges=[]
    mapping={-1: -1}
    delta=0
    epos_tmp=[]
    for edge in edges:
        token=edge[-1] #get the text
        if token=='(' or token==')':
            delta=delta+1
            if token =='(':
                epos_tmp.append(edge[0]+1)
            else:
                epos_tmp.append(edge[0]-1)
        else:
            mapping[edge[0]]=edge[0]-delta
            precessed_edges.append(edge)

    # print(precessed_edges)
    # print(mapping)
    for i in range(len(precessed_edges)):
        precessed_edges[i][0]=mapping[precessed_edges[i][0]]
        precessed_edges[i][1]=mapping[precessed_edges[i][1]]


    epos=[[-1, -1], [-1, -1]]
    # print(epos_tmp)
    epos[0][0]=mapping[epos_tmp[0]]
    epos[0][1]=mapping[epos_tmp[1]]
    epos[1][0]=mapping[epos_tmp[2]]
    epos[1][1]=mapping[epos_tmp[3]]

    return {'sentence': ' '.join(tokens), 'epos': epos, 'tokens': token, 'edges': precessed_edges}


def create_relative_distance(sentence_data):
    e1_distance=[]
    e2_distance=[]
    e1_pos=sentence_data['epos'][0]
    e2_pos=sentence_data['epos'][1]
    for i in range(len(sentence_data['tokens'])):
        if i<e1_pos[0]:
            e1_distance.append(i-e1_pos[0])
        elif i>e1_pos[1]:
            e1_distance.append(i-e1_pos[1])
        else:
            e1_distance.append(0)
        if i<e2_pos[0]:
            e2_distance.append(i-e2_pos[0])
        elif i>e2_pos[1]:
            e2_distance.append(i-e2_pos[1])
        else:
            e2_distance.append(0)
    return e1_distance, e2_distance


def matrix_from_edges(edges, n, initial_val):
    matrix=[[initial_val]*n for i in range(n)]
    for edge in edges:
        matrix[edge[0]][edge[1]]=edge[2]
    return matrix


def get_shortest_path(edges, a, b):
    edges=[[x[0], x[1]] for x in edges]
    und_graph = nx.Graph(edges)
    try:
        und_path=nx.shortest_path(und_graph, source=a, target=b)
    except NetworkXNoPath:
        return [], None
    except NodeNotFound:
        return [], None
    else:
        try: #wwtffffff
          edges=[edges[i] for i in und_path]
          di_graph=nx.DiGraph(edges)
          for node in und_path:
              try:
                  di_path1=nx.shortest_path(di_graph, source=a, target=node)
                  di_path2=nx.shortest_path(di_graph, source=b, target=node)
              except NetworkXNoPath:
                  pass
              else:
                  return und_path, node
        except:
            pass
        return [], None


def path_between_2entity(sentence_data, edges):
    first_e1_pos=sentence_data['epos'][0][0]
    first_e2_pos=sentence_data['epos'][1][0]
    path, grand_parent=get_shortest_path(edges, first_e1_pos, first_e2_pos)
    path_array=['N']*len(edges)
    try:
        grand_parent_pos=path.index(grand_parent)
    except ValueError:
        pass
    else:
        for i, node in enumerate(path):
            path_array[node]=i-grand_parent_pos
    finally:
        return path_array


SKIP=[] #dev only
def get_feature(sentences):
    sentences_data=[]
    for i, s in enumerate(sentences):
        try:
            sentence_data = split_entity_and_create_edges(s)
        except IndexError:
            print('TRY keep entity mark when tokenize for')
            print(s)
            try:
                sentence_data=split_entity_and_create_edges_ver2(s)
            except:
                print('SKIP for')
                print(s)
                sentence_data=None
        except:
            print('SKIP for')
            print(s)
            sentence_data=None
        finally:
            if sentence_data!=None:
                sentences_data.append(sentence_data)
            else:
                SKIP.append(i)


    e1_distance, e2_distance, grammar, shortest_path=[], [], [], []
    for sentence_data in sentences_data:

        #distant
        distance=create_relative_distance(sentence_data)
        e1_distance.append(distance[0])
        e2_distance.append(distance[1])

        edges=sentence_data['edges']
        grammar.append(edges)

        shortest_path.append(path_between_2entity(sentence_data, edges))

    sentences=[x['sentence'] for x in sentences_data]
    return sentences, e1_distance, e2_distance, grammar, shortest_path

def write_log(*messages):
    with open('/content/drive/MyDrive/thesis-relation-extraction-vn/logs.log', 'a') as rf:
      for message in messages:
        print(str(message))

class RE_DataEncoder():
    def __init__(self, vocab_size, max_len, sentences_train, grammar_train, label_train):
        self.max_len=max_len
        self.vocab_size=vocab_size

        #for the sentences
        self.tokenizer = Tokenizer(num_words=vocab_size, filters='!"#$%&()*+,-./:;=?@[]^`{|}~', lower=True, oov_token=1)
        self.tokenizer.fit_on_texts(sentences_train)
        self.word_index = self.tokenizer.word_index
        self.word_size = len(self.word_index) #different from vocab size

        #for the grammar
        grammar_type=[]
        for edges in grammar_train:
            grammar_type.extend([x[2] for x in edges])
        grammar_type=list(set(grammar_type))
        self.grammar2idx={v:i+1 for i, v in enumerate(grammar_type)}

        #for the label
        self.lbencoder = LabelEncoder().fit(label_train)
        self.dict_labels={i: w for i, w in enumerate(list(self.lbencoder.classes_))}


    def encode(self, sentences, e1_distance, e2_distance, grammar, shortest_path):
        #sentence
        sequences = self.tokenizer.texts_to_sequences(sentences)
        sentences_encode = pad_sequences(sequences, maxlen=self.max_len, value=0, padding='post')

        #relative distance
        e1_distance=pad_sequences(e1_distance, maxlen=self.max_len, value=999, padding='post')
        e2_distance=pad_sequences(e2_distance, maxlen=self.max_len, value=999, padding='post')
        e1_distance+=self.max_len
        e2_distance+=self.max_len
        e1_distance[e1_distance==999+self.max_len]=0
        e2_distance[e2_distance==999+self.max_len]=0

        #grammar relantion
        grammar_matrix=[]
        for edge_list in grammar:
            n=len(edge_list)
            for i in range(len(edge_list)):
                try:
                    edge_list[i][2]=self.grammar2idx[edge_list[i][2]]
                except:
                    #edge_list[i][2]=self.grammar2idx['ROOT']
                    edge_list[i][2]=0
            matrix=[x[2] for x in edge_list]
            grammar_matrix.append(matrix)

        grammar_matrix=pad_sequences(grammar_matrix, maxlen=self.max_len, value=0, padding='post')

        #shortest path
        for i in range(len(shortest_path)):
            for j in range(len(shortest_path[i])):
                if shortest_path[i][j]=='N':
                    shortest_path[i][j]=1
                else:
                    shortest_path[i][j]+=self.max_len+1

        shortest_path=pad_sequences(shortest_path, maxlen=self.max_len, value=0, padding='post')

        return sentences_encode, e1_distance, e2_distance, grammar_matrix, shortest_path

    def encode_label(self, label_name):
        label = self.lbencoder.transform(label_name)
        label = to_categorical(label, num_classes=len(set(label_name))) # from keras.utils.np_utils
        label = np.array(label)
        return label

# with open('data/'+'train'+'_features.json', 'r') as r:
#                 data=json.load(r)
# sentences=data['sentences']
# e1_distance=data['e1_distance']
# e2_distance=data['e2_distance']
# grammar=data['grammar']
# shortest_path=data['shortest_path']
# label=data['label']

# tokenizer = Tokenizer(num_words=1, filters='!"#$%&()*+,-./:;=?@[]^`{|}~', lower=True, oov_token=1) # bo dau _
# tokenizer.fit_on_texts(sentences)
# word_index = tokenizer.word_index
# word_size = len(word_index)
# word_size

# import seaborn as sns
# print(max([len(x.split()) for x in sentences]))
# sns.histplot([len(x.split()) for x in sentences]);

SKIP=[]
if __name__ == "__main__":

    vocab_size=11664
    max_len = 40
    read_new_data=False


    for type in ['test']:
        if read_new_data==True:
            data = pd.read_csv('data/'+type+'.csv')
            sentences, e1_distance, e2_distance, grammar, shortest_path=get_feature(data['sentence_vi'])
            label=list(data['relationship'])
            label=[x for i, x in enumerate(label) if i not in SKIP] #dev only
            with open('data/'+type+'_features.json', 'w') as w:
                json.dump({'sentences': sentences,
                        'e1_distance': e1_distance,
                        'e2_distance': e2_distance,
                        'grammar': grammar,
                        'shortest_path': shortest_path,
                        'label': label
                        }, w)
        else:
            with open('data/'+type+'_features.json', 'r') as r:
                data=json.load(r)
            sentences=data['sentences']
            e1_distance=data['e1_distance']
            e2_distance=data['e2_distance']
            grammar=data['grammar']
            shortest_path=data['shortest_path']
            label=data['label']

        if type=='train':
            Encoder = RE_DataEncoder(vocab_size, max_len, sentences, grammar, label)

        sentences_np, e1_distance_np, e2_distance_np, grammar_np, shortest_path_np = Encoder.encode(sentences,
                                                                                                    e1_distance,
                                                                                                    e2_distance,
                                                                                                    grammar,
                                                                                                    shortest_path,
                                                                                                    )
        label_np=Encoder.encode_label(label)

        all_features=np.array([sentences_np, e1_distance_np, e2_distance_np, grammar_np, shortest_path_np])
        np.save('data/X_'+type+'.npy', all_features)
        np.save('data/y_'+type+'.npy', label_np)

        #save encoder class
        with open('data/data_encoder.obj', 'wb') as f:
            pickle.dump(Encoder, f)

# all_features[0].shape