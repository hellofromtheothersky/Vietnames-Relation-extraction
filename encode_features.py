# # -*- coding: utf-8 -*-
# """encode_features.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1rC1Qf3uCIpoFJt9DtMU5GXMTLztk2fSL
# """

# # Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive/')
# # %cd /content/drive/MyDrive/thesis-relation-extraction-vn

# ! pip3 install transformers

import os

import torch
from transformers import AutoTokenizer

import pandas as pd
import numpy as np
import re
import json
import pickle
import torch

from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

class RE_DataEncoder():
    def __init__(self, vocab_size, max_len, org_max_len, grammar_train, label_train):
        self.max_len=max_len
        self.org_max_len=org_max_len
        self.vocab_size=vocab_size

        #for the grammar
        grammar_type=[]
        for edges in grammar_train:
            grammar_type.extend([x[2] for x in edges])
        grammar_type=list(set(grammar_type))
        self.grammar2idx={v:i+1 for i, v in enumerate(grammar_type)}

        #for the label
        self.lbencoder = LabelEncoder().fit(label_train)
        self.dict_labels={i: w for i, w in enumerate(list(self.lbencoder.classes_))}


    def encode_sentences(self, sentences):
        input_ids=[]
        mask_ids=[]
        for sentence in sentences:
            input_id = list(phobert_tokenizer.encode(sentence))[1:-1] # remove CLS and SEP token
            mask_id = [1]*len(input_id)
            while(len(input_id)<self.max_len):
                input_id.append(0)
                mask_id.append(0)
            input_id = input_id[:self.max_len]
            mask_id = mask_id[:self.max_len]

            input_ids.append(input_id)
            mask_ids.append(mask_id)

        return np.array(input_ids), np.array(mask_ids)


    def encode_distances(self, distances):
        distances=pad_sequences(distances, maxlen=self.max_len, value=999, padding='post')
        distances+=self.org_max_len+1
        distances[distances==1000+self.org_max_len]=0
        return distances


    def encode_grammars(self, grammars):
        grammar_matrix=[]
        for edge_list in grammar:
            n=len(edge_list)
            for i in range(len(edge_list)):
                try:
                    edge_list[i][2]=self.grammar2idx[edge_list[i][2]]
                except:
                    #edge_list[i][2]=self.grammar2idx['ROOT']
                    edge_list[i][2]=0
            matrix=[x[2] for x in edge_list]
            grammar_matrix.append(matrix)

        grammar_matrix=pad_sequences(grammar_matrix, maxlen=self.max_len, value=0, padding='post')
        return grammar_matrix


    def encode_SDP(self, shortest_path):
        for i in range(len(shortest_path)):
            for j in range(len(shortest_path[i])):
                if shortest_path[i][j]=='N':
                    shortest_path[i][j]=1
                else:
                    shortest_path[i][j]+=self.org_max_len+1

        shortest_path=pad_sequences(shortest_path, maxlen=self.max_len, value=0, padding='post')
        return shortest_path


    def encode(self, sentences, e1_distances, e2_distances, grammar, shortest_path):
        input_ids, mask_ids=self.encode_sentences(sentences)

        #relative distance
        e1_distances=self.encode_distances(e1_distances)
        e2_distances=self.encode_distances(e2_distances)

        #grammar relantion
        grammar_matrix=self.encode_grammars(grammar)

        #shortest path
        shortest_path=self.encode_SDP(shortest_path)

        return input_ids, mask_ids, e1_distances, e2_distances, grammar_matrix, shortest_path


    def encode_label(self, label_name):
        label = self.lbencoder.transform(label_name)
        label = to_categorical(label, num_classes=len(set(label_name))) # from keras.utils.np_utils
        label = np.array(label)
        return label

if __name__ == "__main__":
    os.chdir('/content/drive/MyDrive/thesis-relation-extraction-vn')
    
    phobert_tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2")

    vocab_size=11664
    max_len = 40
    org_max_len = 90
    read_new_data=False

    for type in ['train', 'test']:
        with open('data/'+type+'_features.json', 'r') as r:
            data=json.load(r)
        sentences=data['sentences']
        e1_distance=data['e1_distance']
        e2_distance=data['e2_distance']
        grammar=data['grammar']
        shortest_path=data['shortest_path']
        label=data['label']

        if type=='train':
            Encoder = RE_DataEncoder(vocab_size, max_len, org_max_len, grammar, label)

        input_ids_np , mask_ids_np, e1_distance_np, e2_distance_np, grammar_np, shortest_path_np = Encoder.encode(sentences,
                                                                                                    e1_distance,
                                                                                                    e2_distance,
                                                                                                    grammar,
                                                                                                    shortest_path,
                                                                                                    )
        label_np=Encoder.encode_label(label)

        all_features=np.array([input_ids_np , mask_ids_np, e1_distance_np, e2_distance_np, grammar_np, shortest_path_np])
        np.save('data/X_'+type+'.npy', all_features)
        np.save('data/y_'+type+'.npy', label_np)

        #save encoder class
        with open('data/data_encoder.obj', 'wb') as f:
            pickle.dump(Encoder, f)

# #FIND MAX LEN
# import seaborn as sns
# print(max([len(x.split()) for x in sentences]))
# sns.histplot([len(x.split()) for x in sentences]);